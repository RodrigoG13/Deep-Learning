{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.1.2+cu121\n",
      "GPU available: True\n",
      "Epoch 1, Loss: 4.850700923374721, Train Accuracy: 29.325842696629213%, Val Accuracy: 49.327354260089685%\n",
      "Epoch 2, Loss: 4.566884585789272, Train Accuracy: 41.17977528089887%, Val Accuracy: 50.44843049327354%\n",
      "Epoch 3, Loss: 4.290960907936096, Train Accuracy: 46.01123595505618%, Val Accuracy: 50.672645739910315%\n",
      "Epoch 4, Loss: 4.046322797025953, Train Accuracy: 47.41573033707865%, Val Accuracy: 51.12107623318386%\n",
      "Epoch 5, Loss: 3.795798029218401, Train Accuracy: 48.03370786516854%, Val Accuracy: 51.79372197309417%\n",
      "Epoch 6, Loss: 3.5325985295431956, Train Accuracy: 48.48314606741573%, Val Accuracy: 52.690582959641254%\n",
      "Epoch 7, Loss: 3.240062747682844, Train Accuracy: 50.842696629213485%, Val Accuracy: 54.48430493273543%\n",
      "Epoch 8, Loss: 3.112003445625305, Train Accuracy: 53.59550561797753%, Val Accuracy: 56.053811659192824%\n",
      "Epoch 9, Loss: 2.922296941280365, Train Accuracy: 54.943820224719104%, Val Accuracy: 56.72645739910314%\n",
      "Epoch 10, Loss: 2.7286645088876997, Train Accuracy: 55.50561797752809%, Val Accuracy: 60.08968609865471%\n",
      "Epoch 11, Loss: 2.611169695854187, Train Accuracy: 55.337078651685395%, Val Accuracy: 60.53811659192825%\n",
      "Epoch 12, Loss: 2.435399668557303, Train Accuracy: 59.26966292134831%, Val Accuracy: 60.08968609865471%\n",
      "Epoch 13, Loss: 2.2950878739356995, Train Accuracy: 59.157303370786515%, Val Accuracy: 60.31390134529148%\n",
      "Epoch 14, Loss: 2.2117146168436324, Train Accuracy: 60.1123595505618%, Val Accuracy: 59.64125560538117%\n",
      "Epoch 15, Loss: 2.127932846546173, Train Accuracy: 59.26966292134831%, Val Accuracy: 60.31390134529148%\n",
      "Epoch 16, Loss: 2.113661216838019, Train Accuracy: 59.60674157303371%, Val Accuracy: 60.31390134529148%\n",
      "Epoch 17, Loss: 1.9972224576132638, Train Accuracy: 60.337078651685395%, Val Accuracy: 61.88340807174888%\n",
      "Epoch 18, Loss: 1.977331199816295, Train Accuracy: 59.438202247191015%, Val Accuracy: 60.53811659192825%\n",
      "Epoch 19, Loss: 1.8223932896341597, Train Accuracy: 61.96629213483146%, Val Accuracy: 61.210762331838566%\n",
      "Epoch 20, Loss: 1.7702172228268214, Train Accuracy: 62.58426966292135%, Val Accuracy: 61.43497757847533%\n",
      "Epoch 21, Loss: 1.7498265377112798, Train Accuracy: 62.247191011235955%, Val Accuracy: 61.65919282511211%\n",
      "Epoch 22, Loss: 1.7515546807221003, Train Accuracy: 62.97752808988764%, Val Accuracy: 63.00448430493274%\n",
      "Epoch 23, Loss: 1.6748029589653015, Train Accuracy: 64.49438202247191%, Val Accuracy: 62.780269058295964%\n",
      "Epoch 24, Loss: 1.5928311220237188, Train Accuracy: 63.93258426966292%, Val Accuracy: 62.780269058295964%\n",
      "Epoch 25, Loss: 1.5814997383526392, Train Accuracy: 66.17977528089888%, Val Accuracy: 64.57399103139014%\n",
      "Epoch 26, Loss: 1.5754669734409876, Train Accuracy: 62.752808988764045%, Val Accuracy: 64.57399103139014%\n",
      "Epoch 27, Loss: 1.5204873893942152, Train Accuracy: 66.01123595505618%, Val Accuracy: 65.02242152466367%\n",
      "Epoch 28, Loss: 1.526065754038947, Train Accuracy: 65.84269662921348%, Val Accuracy: 63.90134529147982%\n",
      "Epoch 29, Loss: 1.4427555458886283, Train Accuracy: 66.01123595505618%, Val Accuracy: 63.67713004484305%\n",
      "Epoch 30, Loss: 1.439511924982071, Train Accuracy: 66.40449438202248%, Val Accuracy: 65.02242152466367%\n",
      "Epoch 31, Loss: 1.4054098256996699, Train Accuracy: 66.34831460674157%, Val Accuracy: 65.24663677130044%\n",
      "Epoch 32, Loss: 1.379670547587531, Train Accuracy: 66.85393258426966%, Val Accuracy: 66.14349775784753%\n",
      "Epoch 33, Loss: 1.3600825837680273, Train Accuracy: 67.30337078651685%, Val Accuracy: 65.69506726457399%\n",
      "Epoch 34, Loss: 1.3674592971801758, Train Accuracy: 67.75280898876404%, Val Accuracy: 64.57399103139014%\n",
      "Epoch 35, Loss: 1.320238390139171, Train Accuracy: 67.41573033707866%, Val Accuracy: 66.3677130044843%\n",
      "Epoch 36, Loss: 1.3307330054896218, Train Accuracy: 67.80898876404494%, Val Accuracy: 66.59192825112108%\n",
      "Epoch 37, Loss: 1.3171935549804143, Train Accuracy: 68.42696629213484%, Val Accuracy: 65.69506726457399%\n",
      "Epoch 38, Loss: 1.274234795144626, Train Accuracy: 69.10112359550561%, Val Accuracy: 65.24663677130044%\n",
      "Epoch 39, Loss: 1.2500779117856706, Train Accuracy: 69.43820224719101%, Val Accuracy: 66.3677130044843%\n",
      "Epoch 40, Loss: 1.2164870074817113, Train Accuracy: 69.71910112359551%, Val Accuracy: 66.59192825112108%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 102\u001b[0m\n\u001b[1;32m    100\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m    101\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m--> 102\u001b[0m running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    103\u001b[0m _, predicted \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(outputs\u001b[38;5;241m.\u001b[39mdata, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    104\u001b[0m total \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import models, io\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch import nn, optim\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from PIL import Image\n",
    "\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "print(\"GPU available:\", torch.cuda.is_available())\n",
    "\n",
    "labels_df = pd.read_csv('labels/labels.csv')\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "labels_df['label'] = label_encoder.fit_transform(labels_df['label'])\n",
    "\n",
    "base_dir = 'images/'\n",
    "\n",
    "labels_df['image_path'] = labels_df['image_name'].apply(lambda x: os.path.join(base_dir, x))\n",
    "\n",
    "train_df, test_df = train_test_split(labels_df, test_size=0.2, random_state=42)\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(299),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(30),\n",
    "    transforms.RandomAffine(degrees=30, translate=(0.1, 0.1), scale=(0.8, 1.2)),\n",
    "    transforms.RandomPerspective(distortion_scale=0.3, p=0.1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize(299),\n",
    "    transforms.CenterCrop(299),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, dataframe, transform=None):\n",
    "        self.df = dataframe\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.df.iloc[idx]['image_path']\n",
    "        image = Image.open(img_name).convert('RGB')\n",
    "        label = self.df.iloc[idx]['label']\n",
    "        label = torch.tensor(label, dtype=torch.long)\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "\n",
    "train_dataset = CustomImageDataset(train_df, transform=train_transform)\n",
    "test_dataset = CustomImageDataset(test_df, transform=test_transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Cargar el modelo InceptionV3 preentrenado\n",
    "model = models.inception_v3(weights=models.Inception_V3_Weights.IMAGENET1K_V1)\n",
    "model.aux_logits = True\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = nn.Linear(num_ftrs, len(label_encoder.classes_))\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-5)\n",
    "\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5, verbose=True)\n",
    "\n",
    "train_losses = []\n",
    "train_accuracy = []\n",
    "val_accuracy = []\n",
    "\n",
    "best_acc = 0.0\n",
    "\n",
    "# Entrenamiento\n",
    "for epoch in range(70):\n",
    "    model.train()\n",
    "    model.aux_logits = True\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs, aux_outputs = model(images)\n",
    "        loss1 = criterion(outputs, labels)\n",
    "        loss2 = criterion(aux_outputs, labels)\n",
    "        loss = loss1 + 0.4 * loss2\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    train_losses.append(running_loss / len(train_loader))\n",
    "    train_accuracy.append(100 * correct / total)\n",
    "\n",
    "    # EvaluaciÃ³n\n",
    "    model.eval()\n",
    "    model.aux_logits = False\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            if outputs.dim() == 1:\n",
    "                outputs = outputs.unsqueeze(0)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            val_total += labels.size(0)\n",
    "            val_correct += (predicted == labels).sum().item()\n",
    "\n",
    "    val_acc = 100 * val_correct / val_total\n",
    "    val_accuracy.append(val_acc)\n",
    "\n",
    "    print(f'Epoch {epoch+1}, Loss: {running_loss/len(train_loader)}, Train Accuracy: {100 * correct / total}%, Val Accuracy: {val_acc}%')\n",
    "\n",
    "    scheduler.step(val_loss / len(test_loader))\n",
    "\n",
    "    # Checkpointing\n",
    "    if val_acc > best_acc:\n",
    "        best_acc = val_acc\n",
    "        torch.save(model.state_dict(), 'best_model.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rodrigo/.local/lib/python3.10/site-packages/torchvision/models/inception.py:43: FutureWarning: The default weight initialization of inception_v3 will be changed in future releases of torchvision. If you wish to keep the old behavior (which leads to long initialization times due to scipy/scipy#11299), please set init_weights=True.\n",
      "  warnings.warn(\n",
      "/home/rodrigo/.local/lib/python3.10/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/home/rodrigo/.local/lib/python3.10/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/home/rodrigo/.local/lib/python3.10/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import os\n",
    "from torchvision import io, transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.models import inception_v3\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "model_path = 'best_model.pth'\n",
    "model = inception_v3(weights=None)\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = torch.nn.Linear(num_ftrs, len(label_encoder.classes_))\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "model.eval()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "test_df = pd.read_csv('labels/test.csv')\n",
    "base_dir = 'images/'\n",
    "test_df['image_path'] = test_df['image_name'].apply(lambda x: os.path.join(base_dir, x))\n",
    "\n",
    "\n",
    "class TestImageDataset(Dataset):\n",
    "    def __init__(self, dataframe, transform=None):\n",
    "        self.df = dataframe\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.df.iloc[idx]['image_path']\n",
    "        image = io.read_image(img_name).float() / 255.0\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, self.df.iloc[idx]['image_name'], self.df.iloc[idx]['place'], self.df.iloc[idx]['date']\n",
    "\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize((299, 299)),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "\n",
    "test_dataset = TestImageDataset(test_df, transform=test_transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "\n",
    "results = []\n",
    "with torch.no_grad():\n",
    "    for images, names, places, dates in test_loader:\n",
    "        images = images.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        labels = [label_encoder.inverse_transform([pred.item()])[0] for pred in predicted]\n",
    "\n",
    "        for name, place, date, label in zip(names, places, dates, labels):\n",
    "            results.append([name, place, date, label])\n",
    "\n",
    "\n",
    "results_df = pd.DataFrame(results, columns=['image_name', 'place', 'date', 'label'])\n",
    "results_df.to_csv('resultado.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
