{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.1.2+cu121\n",
      "GPU available: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rodrigo/.local/lib/python3.10/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 4.757251569202968, Train Accuracy: 35.28089887640449%, Val Accuracy: 41.25560538116592%\n",
      "Actualización!\n",
      "Epoch 2, Loss: 4.470181005341666, Train Accuracy: 42.247191011235955%, Val Accuracy: 51.5695067264574%\n",
      "Actualización!\n",
      "Epoch 3, Loss: 4.190471759864262, Train Accuracy: 45.95505617977528%, Val Accuracy: 53.81165919282511%\n",
      "Actualización!\n",
      "Epoch 4, Loss: 3.896992496081761, Train Accuracy: 49.21348314606742%, Val Accuracy: 55.38116591928251%\n",
      "Actualización!\n",
      "Epoch 5, Loss: 3.653391284602029, Train Accuracy: 49.7752808988764%, Val Accuracy: 54.48430493273543%\n",
      "Epoch 6, Loss: 3.4081968324525014, Train Accuracy: 50.2247191011236%, Val Accuracy: 56.2780269058296%\n",
      "Actualización!\n",
      "Epoch 7, Loss: 3.205998820917947, Train Accuracy: 52.92134831460674%, Val Accuracy: 57.399103139013455%\n",
      "Actualización!\n",
      "Epoch 8, Loss: 3.0441390701702664, Train Accuracy: 53.53932584269663%, Val Accuracy: 60.762331838565025%\n",
      "Actualización!\n",
      "Epoch 9, Loss: 2.8967044183186124, Train Accuracy: 53.146067415730336%, Val Accuracy: 58.74439461883408%\n",
      "Epoch 10, Loss: 2.733769714832306, Train Accuracy: 55.2247191011236%, Val Accuracy: 58.29596412556054%\n",
      "Epoch 11, Loss: 2.6031005552836826, Train Accuracy: 55.28089887640449%, Val Accuracy: 60.53811659192825%\n",
      "Epoch 12, Loss: 2.464810456548418, Train Accuracy: 58.48314606741573%, Val Accuracy: 61.43497757847533%\n",
      "Actualización!\n",
      "Epoch 13, Loss: 2.3092528326170787, Train Accuracy: 59.438202247191015%, Val Accuracy: 63.228699551569505%\n",
      "Actualización!\n",
      "Epoch 14, Loss: 2.253538374389921, Train Accuracy: 59.157303370786515%, Val Accuracy: 62.55605381165919%\n",
      "Epoch 15, Loss: 2.102006724902562, Train Accuracy: 59.71910112359551%, Val Accuracy: 62.10762331838565%\n",
      "Epoch 16, Loss: 2.0488798448017667, Train Accuracy: 60.28089887640449%, Val Accuracy: 61.88340807174888%\n",
      "Epoch 17, Loss: 1.961791340793882, Train Accuracy: 60.95505617977528%, Val Accuracy: 61.88340807174888%\n",
      "Epoch 18, Loss: 1.8849128016403742, Train Accuracy: 61.853932584269664%, Val Accuracy: 62.10762331838565%\n",
      "Epoch 19, Loss: 1.8136162119252341, Train Accuracy: 63.764044943820224%, Val Accuracy: 61.88340807174888%\n",
      "Epoch 20, Loss: 1.7741258016654424, Train Accuracy: 61.853932584269664%, Val Accuracy: 61.65919282511211%\n",
      "Epoch 21, Loss: 1.6951973140239716, Train Accuracy: 64.04494382022472%, Val Accuracy: 63.228699551569505%\n",
      "Epoch 22, Loss: 1.738634501184736, Train Accuracy: 61.91011235955056%, Val Accuracy: 63.228699551569505%\n",
      "Epoch 23, Loss: 1.648267196757453, Train Accuracy: 63.37078651685393%, Val Accuracy: 62.55605381165919%\n",
      "Epoch 24, Loss: 1.594459980726242, Train Accuracy: 64.49438202247191%, Val Accuracy: 62.55605381165919%\n",
      "Epoch 25, Loss: 1.5255798186574663, Train Accuracy: 65.67415730337079%, Val Accuracy: 64.34977578475336%\n",
      "Actualización!\n",
      "Epoch 26, Loss: 1.4916618125779288, Train Accuracy: 64.9438202247191%, Val Accuracy: 63.90134529147982%\n",
      "Epoch 27, Loss: 1.472825778382165, Train Accuracy: 65.84269662921348%, Val Accuracy: 61.43497757847533%\n",
      "Epoch 28, Loss: 1.4334480166435242, Train Accuracy: 66.17977528089888%, Val Accuracy: 61.88340807174888%\n",
      "Epoch 29, Loss: 1.4397327431610651, Train Accuracy: 66.68539325842697%, Val Accuracy: 65.47085201793722%\n",
      "Actualización!\n",
      "Epoch 30, Loss: 1.376817945923124, Train Accuracy: 66.79775280898876%, Val Accuracy: 64.57399103139014%\n",
      "Epoch 31, Loss: 1.3723933824471064, Train Accuracy: 67.75280898876404%, Val Accuracy: 64.79820627802691%\n",
      "Epoch 32, Loss: 1.3919825511319297, Train Accuracy: 66.85393258426966%, Val Accuracy: 61.210762331838566%\n",
      "Epoch 33, Loss: 1.3349981222833907, Train Accuracy: 68.37078651685393%, Val Accuracy: 63.45291479820628%\n",
      "Epoch 34, Loss: 1.2857151627540588, Train Accuracy: 69.15730337078652%, Val Accuracy: 67.04035874439462%\n",
      "Actualización!\n",
      "Epoch 35, Loss: 1.2773087407861436, Train Accuracy: 69.5505617977528%, Val Accuracy: 66.81614349775785%\n",
      "Epoch 36, Loss: 1.2610934632165092, Train Accuracy: 68.20224719101124%, Val Accuracy: 65.24663677130044%\n",
      "Epoch 37, Loss: 1.2374712505510874, Train Accuracy: 68.98876404494382%, Val Accuracy: 65.02242152466367%\n",
      "Epoch 38, Loss: 1.2049189337662287, Train Accuracy: 70.3932584269663%, Val Accuracy: 66.14349775784753%\n",
      "Epoch 39, Loss: 1.1888921856880188, Train Accuracy: 70.61797752808988%, Val Accuracy: 66.81614349775785%\n",
      "Epoch 40, Loss: 1.1964681914874487, Train Accuracy: 70.56179775280899%, Val Accuracy: 67.48878923766816%\n",
      "Actualización!\n",
      "Epoch 41, Loss: 1.1865090557507105, Train Accuracy: 71.01123595505618%, Val Accuracy: 67.04035874439462%\n",
      "Epoch 42, Loss: 1.1206530715738023, Train Accuracy: 72.58426966292134%, Val Accuracy: 65.24663677130044%\n",
      "Epoch 43, Loss: 1.1277661131961005, Train Accuracy: 71.91011235955057%, Val Accuracy: 67.26457399103138%\n",
      "Epoch 44, Loss: 1.1405745957578932, Train Accuracy: 70.61797752808988%, Val Accuracy: 66.3677130044843%\n",
      "Epoch 45, Loss: 1.1265983496393477, Train Accuracy: 72.69662921348315%, Val Accuracy: 67.48878923766816%\n",
      "Epoch 46, Loss: 1.080509398664747, Train Accuracy: 74.5505617977528%, Val Accuracy: 68.60986547085201%\n",
      "Actualización!\n",
      "Epoch 47, Loss: 1.1119131680045808, Train Accuracy: 72.69662921348315%, Val Accuracy: 66.59192825112108%\n",
      "Epoch 48, Loss: 1.1046096171651567, Train Accuracy: 71.91011235955057%, Val Accuracy: 64.12556053811659%\n",
      "Epoch 49, Loss: 1.0863738081284933, Train Accuracy: 73.42696629213484%, Val Accuracy: 66.59192825112108%\n",
      "Epoch 50, Loss: 1.0421662096466338, Train Accuracy: 74.5505617977528%, Val Accuracy: 67.26457399103138%\n",
      "Epoch 51, Loss: 1.0030976768050874, Train Accuracy: 74.7752808988764%, Val Accuracy: 65.02242152466367%\n",
      "Epoch 52, Loss: 1.0122416147163935, Train Accuracy: 73.48314606741573%, Val Accuracy: 67.71300448430493%\n",
      "Epoch 53, Loss: 1.0101693932499205, Train Accuracy: 75.0561797752809%, Val Accuracy: 66.59192825112108%\n",
      "Epoch 54, Loss: 0.9577205904892513, Train Accuracy: 75.95505617977528%, Val Accuracy: 65.69506726457399%\n",
      "Epoch 55, Loss: 0.970857139144625, Train Accuracy: 75.95505617977528%, Val Accuracy: 67.26457399103138%\n",
      "Epoch 56, Loss: 1.0156822332314082, Train Accuracy: 74.15730337078652%, Val Accuracy: 66.3677130044843%\n",
      "Epoch 57, Loss: 0.9752261447055, Train Accuracy: 75.28089887640449%, Val Accuracy: 68.16143497757848%\n",
      "Epoch 58, Loss: 0.9207205516951424, Train Accuracy: 77.64044943820225%, Val Accuracy: 68.60986547085201%\n",
      "Epoch 59, Loss: 0.941672648702349, Train Accuracy: 77.19101123595506%, Val Accuracy: 67.71300448430493%\n",
      "Epoch 60, Loss: 0.9451902679034642, Train Accuracy: 77.35955056179775%, Val Accuracy: 67.04035874439462%\n",
      "Epoch 61, Loss: 0.8973743042775563, Train Accuracy: 77.30337078651685%, Val Accuracy: 68.38565022421524%\n",
      "Epoch 62, Loss: 0.9127198798315865, Train Accuracy: 77.80898876404494%, Val Accuracy: 68.16143497757848%\n",
      "Epoch 63, Loss: 0.9223439821175167, Train Accuracy: 77.64044943820225%, Val Accuracy: 66.14349775784753%\n",
      "Epoch 64, Loss: 0.8951627356665475, Train Accuracy: 78.70786516853933%, Val Accuracy: 68.60986547085201%\n",
      "Epoch 65, Loss: 0.8934651868683952, Train Accuracy: 78.42696629213484%, Val Accuracy: 66.81614349775785%\n",
      "Epoch 66, Loss: 0.8871173092297145, Train Accuracy: 77.86516853932584%, Val Accuracy: 66.81614349775785%\n",
      "Epoch 67, Loss: 0.8700666576623917, Train Accuracy: 79.04494382022472%, Val Accuracy: 66.3677130044843%\n",
      "Epoch 68, Loss: 0.8152145892381668, Train Accuracy: 81.06741573033707%, Val Accuracy: 66.14349775784753%\n",
      "Epoch 69, Loss: 0.8563687524625233, Train Accuracy: 79.43820224719101%, Val Accuracy: 66.14349775784753%\n",
      "Epoch 70, Loss: 0.8313914281981332, Train Accuracy: 79.6067415730337%, Val Accuracy: 68.60986547085201%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import models, io\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch import nn, optim\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "print(\"GPU available:\", torch.cuda.is_available())\n",
    "\n",
    "labels_df = pd.read_csv('labels/labels.csv')\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "labels_df['label'] = label_encoder.fit_transform(labels_df['label'])\n",
    "\n",
    "base_dir = 'images/'\n",
    "\n",
    "labels_df['image_path'] = labels_df['image_name'].apply(lambda x: os.path.join(base_dir, x))\n",
    "\n",
    "train_df, test_df = train_test_split(labels_df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Transformaciones ajustadas para el entrenamiento\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(299),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(20),  # Ángulo reducido\n",
    "    transforms.ColorJitter(contrast=1.2),  # Contraste reducido\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Transformaciones ajustadas para la evaluación\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize(299),\n",
    "    transforms.CenterCrop(299),\n",
    "    transforms.ColorJitter(contrast=1.2),  # Contraste reducido\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, dataframe, transform=None):\n",
    "        self.df = dataframe\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.df.iloc[idx]['image_path']\n",
    "        image = io.read_image(img_name).float() / 255.0\n",
    "        label = self.df.iloc[idx]['label']\n",
    "        label = torch.tensor(label, dtype=torch.long)\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "\n",
    "train_dataset = CustomImageDataset(train_df, transform=train_transform)\n",
    "test_dataset = CustomImageDataset(test_df, transform=test_transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Cargar el modelo InceptionV3 preentrenado\n",
    "model = models.inception_v3(weights=models.Inception_V3_Weights.IMAGENET1K_V1)\n",
    "model.aux_logits = True\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = nn.Linear(num_ftrs, len(label_encoder.classes_))\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-5)\n",
    "\n",
    "train_losses = []\n",
    "train_accuracy = []\n",
    "val_accuracy = []\n",
    "\n",
    "best_acc = 0.0\n",
    "\n",
    "# Entrenamiento\n",
    "for epoch in range(70):\n",
    "    model.train()\n",
    "    model.aux_logits = True\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs, aux_outputs = model(images)\n",
    "        loss1 = criterion(outputs, labels)\n",
    "        loss2 = criterion(aux_outputs, labels)\n",
    "        loss = loss1 + 0.4 * loss2\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    train_losses.append(running_loss / len(train_loader))\n",
    "    train_accuracy.append(100 * correct / total)\n",
    "\n",
    "    # Evaluación\n",
    "    model.eval()\n",
    "    model.aux_logits = False\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            if outputs.dim() == 1:\n",
    "                outputs = outputs.unsqueeze(0)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            val_total += labels.size(0)\n",
    "            val_correct += (predicted == labels).sum().item()\n",
    "\n",
    "    val_acc = 100 * val_correct / val_total\n",
    "    val_accuracy.append(val_acc)\n",
    "\n",
    "    print(f'Epoch {epoch+1}, Loss: {running_loss/len(train_loader)}, Train Accuracy: {100 * correct / total}%, Val Accuracy: {val_acc}%')\n",
    "\n",
    "    # Checkpointing\n",
    "    if val_acc > best_acc:\n",
    "        best_acc = val_acc\n",
    "        print(\"Actualización!\")\n",
    "        torch.save(model.state_dict(), 'best_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rodrigo/.local/lib/python3.10/site-packages/torchvision/models/inception.py:43: FutureWarning: The default weight initialization of inception_v3 will be changed in future releases of torchvision. If you wish to keep the old behavior (which leads to long initialization times due to scipy/scipy#11299), please set init_weights=True.\n",
      "  warnings.warn(\n",
      "/home/rodrigo/.local/lib/python3.10/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/home/rodrigo/.local/lib/python3.10/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/home/rodrigo/.local/lib/python3.10/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import os\n",
    "from torchvision import io, transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.models import inception_v3\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "model_path = 'best_model.pth'\n",
    "model = inception_v3(weights=None)\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = torch.nn.Linear(num_ftrs, len(label_encoder.classes_))\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "model.eval()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "test_df = pd.read_csv('labels/test.csv')\n",
    "base_dir = 'images/'\n",
    "test_df['image_path'] = test_df['image_name'].apply(lambda x: os.path.join(base_dir, x))\n",
    "\n",
    "\n",
    "class TestImageDataset(Dataset):\n",
    "    def __init__(self, dataframe, transform=None):\n",
    "        self.df = dataframe\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.df.iloc[idx]['image_path']\n",
    "        image = io.read_image(img_name).float() / 255.0\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, self.df.iloc[idx]['image_name'], self.df.iloc[idx]['place'], self.df.iloc[idx]['date']\n",
    "\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize((299, 299)),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "\n",
    "test_dataset = TestImageDataset(test_df, transform=test_transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "\n",
    "results = []\n",
    "with torch.no_grad():\n",
    "    for images, names, places, dates in test_loader:\n",
    "        images = images.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        labels = [label_encoder.inverse_transform([pred.item()])[0] for pred in predicted]\n",
    "\n",
    "        for name, place, date, label in zip(names, places, dates, labels):\n",
    "            results.append([name, place, date, label])\n",
    "\n",
    "\n",
    "results_df = pd.DataFrame(results, columns=['image_name', 'place', 'date', 'label'])\n",
    "results_df.to_csv('resultado.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
