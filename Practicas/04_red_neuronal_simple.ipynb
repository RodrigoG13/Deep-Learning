{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Red neuronal simple\n",
    "\n",
    "En este notebook programaremos una red neuronal simple utilizando numpy. El objetivo es que comprendamos la diferencia con el perceptrón. A diferencia del ejercicio anterior en este usaremos funciones de numpy.\n",
    "\n",
    "- *numpy.dot(a, b, out=None)* Dot product of two arrays. Parameters: a array_like First argument, b array_like Second argument.\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/irvingvasquez/cv2course_intro_nn/blob/master/02_red_neuronal_simple.ipynb)\n",
    "\n",
    "@juan1rving\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calcular producto punto\n",
    "\n",
    "El primer paso es calcular el logit, *h*, a partir del producto punto. La fórmula explícita es la siguiente:\n",
    "\n",
    "$$ h = W X +b $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_combination(X , W , b):\n",
    "    h = np.dot(W, X) + b\n",
    "    return h"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Función de activación\n",
    "\n",
    "Para este ejemplo utilizaremos la función sigmoide como función de activación.\n",
    "\n",
    "$$ f(x) =  \\frac{\\mathrm{1} }{\\mathrm{1} + e^{-x} }  $$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(h:np.float64) -> np.float64:\n",
    "    \"\"\"\n",
    "    Compute de sigmoid function for h\n",
    "\n",
    "    Parameters:\n",
    "    h (numpy.float64): Number to which sigmoid function is applied\n",
    "\n",
    "    Returns:\n",
    "    numpy.float64: Output to sigmoid function\n",
    "    \"\"\"\n",
    "    sg = 1 / (1 + np.exp(-h))\n",
    "    return sg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neurona simple\n",
    "\n",
    "Implementación de la neurona simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neuron(W:np.ndarray, X:np.ndarray, b:float, activation) -> np.float64:\n",
    "    \"\"\"Fuction that simulates a simple neural network\n",
    "\n",
    "    Args:\n",
    "        W (np.ndarray): Model parameter matrix\n",
    "        X (np.ndarray): Neuron inputs\n",
    "        b (float): bias\n",
    "        activation: activation function\n",
    "\n",
    "    Returns:\n",
    "        np.float64: Neural Network prediction\n",
    "    \"\"\"\n",
    "    h = linear_combination(W, X, b)\n",
    "    return activation(h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probar la red\n",
    "\n",
    "Ahora definamos unos pesos y veamos el resultado de una pasada frontal (fordward pass)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\t***SIMPLE NEURAL NETWORK WITH NUMPY***\n",
      "Inputs: [ 0.7 -0.3]\n",
      "Weights: [0.1 0.8]\n",
      "Bias: -0.1\n",
      "Output with sigmoid function: 0.4329070950345457\n"
     ]
    }
   ],
   "source": [
    "# Definamos unos pesos y sesgo\n",
    "inputs = np.array([0.7, -0.3])\n",
    "weights = np.array([0.1, 0.8])\n",
    "bias = -0.1\n",
    "activation_function = sigmoid\n",
    "output = neuron(weights, inputs, bias, activation_function)\n",
    "\n",
    "print(\"\\t\\t\\t***SIMPLE NEURAL NETWORK WITH NUMPY***\")\n",
    "print(f\"Inputs: {inputs}\")\n",
    "print(f\"Weights: {weights}\")\n",
    "print(f\"Bias: {bias}\")\n",
    "print(f'Output with sigmoid function: {output}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tanh(h:np.float64):\n",
    "    \"\"\"\n",
    "    Compute the hyperbolic tangent for h.\n",
    "\n",
    "    Parameters:\n",
    "    h (numpy.float64): Input number.\n",
    "\n",
    "    Returns:\n",
    "    numpy.np.float64: Hyperbolic tangent evaluation\n",
    "    \"\"\"\n",
    "    return np.tanh(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\t***SIMPLE NEURAL NETWORK WITH NUMPY***\n",
      "Inputs: [ 0.7 -0.3]\n",
      "Weights: [0.1 0.8]\n",
      "Bias: -0.1\n",
      "Output with hyperbolic tangent function: -0.26362483547220333\n"
     ]
    }
   ],
   "source": [
    "activation_function = tanh\n",
    "output = neuron(weights, inputs, bias, activation_function)\n",
    "\n",
    "print(\"\\t\\t\\t***SIMPLE NEURAL NETWORK WITH NUMPY***\")\n",
    "print(f\"Inputs: {inputs}\")\n",
    "print(f\"Weights: {weights}\")\n",
    "print(f\"Bias: {bias}\")\n",
    "print(f'Output with hyperbolic tangent function: {output}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contesta: ¿La tanh y sigmoide producen las mismas salidas?\n",
    "\n",
    "No, ambas funciones arrojan diferentes resultados a su salida.\n",
    "Si bien, ambas reciben el mismo parámetro h a sus entradas, dichas funciones se definen de manera diferente. Además de que el rango de sigmoide(h) pertenece a [0,1], mientras que el de tanh(h) pertenece a [-1,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusiones\n",
    "\n",
    "En esta práctica, observamos cómo diferentes funciones de activación afectan la salida de una red neuronal simple implementada con NumPy. Al emplear la función sigmoide como función de activación, los resultados estaban restringidos en el rango de 0 a 1, mientras que al utilizar la función tangente hiperbólica, los valores de salida se encontraban en el rango de -1 a 1.\n",
    "\n",
    "Esto resalta la influencia significativa que tiene la elección de la función de activación en el comportamiento y la capacidad de la red neuronal para modelar relaciones entre las entradas y las salidas. Además, ilustra cómo diferentes funciones de activación pueden ser más adecuadas para diferentes tipos de problemas o datos."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nni",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "cbad788490f55b163920bee5e9d5e0cba00db5905dc94f4bdbe0011e55bf465f"
   }
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
