{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Red neuronal multicapa\n",
    "\n",
    "Una red neuronal multicapa tiene capas ocultas entre la entrada y la salida. A este tipo de red también se le conoce como perceptrón multicapa (MLP por sus siglas en inglés de multilayer perceptron)\n",
    "\n",
    "En este ejercicio implementaremos una red neuronal multicapa usando únicamente numpy.\n",
    "\n",
    "INSTRUCCIONES: Completa el código faltante.\n",
    "\n",
    "### Alumno: Rodrigo Gerardo Trejo Arriaga"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importamos paquetes\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funciones necesarias\n",
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Función sigmoide\n",
    "    \"\"\"\n",
    "    return 1/(1+np.exp(-x))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definamos la arquitectura de la red\n",
    "\n",
    "<img src=\"files/test1.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Especifica el tamaño de las capas de acuerdo al diagrama.\n",
    "N_input = 4\n",
    "N_hidden = 3\n",
    "N_output = 2\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definir los pesos\n",
    "\n",
    "Recordemos que los pesos ahora los representamos con matrices y utilizaremos el producton de la siguiente forma:\n",
    "\n",
    "$$\n",
    "h = XW\n",
    "$$\n",
    "\n",
    "tal que las entradas son\n",
    "\n",
    "$$ X = \\begin{bmatrix} x_1 & x_2 & x_3 \\end{bmatrix} $$\n",
    "\n",
    "y\n",
    "\n",
    "$$W = \\begin{bmatrix}\n",
    "w_{1,1} & w_{1,2}\n",
    " \\\\\n",
    "w_{2,1} & w_{2,2}\n",
    " \\\\\n",
    "w_{3,1} & w_{3,2}\n",
    " \\\\\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "recuerda que los renglones se relacionan con las entradas y las columnas con los nodos intermedios. \n",
    "\n",
    "Como herramienta usaremos la función normal. Verifica la documentación oficial de la función.\n",
    "\n",
    "> numpy.random.normal(loc=0.0, scale=1.0, size=None)\n",
    "\n",
    "TODO: construye a contrinuación matrices con valores aleatorios para la matriz de pesos entre la entrada y la oculta y para la oculta y la salida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = 0\n",
    "stdev = 0.5\n",
    "\n",
    "W_1 = np.random.normal(loc=mean, scale=stdev, size=(N_input, N_hidden))\n",
    "W_2 = np.random.normal(loc=mean, scale=stdev, size=(N_hidden, N_output))\n",
    "X = np.random.rand(1, N_input)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calcular la salida\n",
    "\n",
    "Recordemos de la lección que\n",
    "\n",
    "$$ h = X W $$\n",
    "\n",
    "$$ y = f(H) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "salida de la capa oculta:  [[0.71599158 0.48250233 0.44073576]]\n",
      "salida de la red:  [[0.36523534 0.32052748]]\n"
     ]
    }
   ],
   "source": [
    "H_1 = np.dot(X, W_1)\n",
    "A_1 = sigmoid(H_1)\n",
    "\n",
    "print(\"salida de la capa oculta: \", A_1)\n",
    "\n",
    "H_2 = np.dot(A_1, W_2)\n",
    "Y_gorrito = sigmoid(H_2)\n",
    "\n",
    "print(\"salida de la red: \", Y_gorrito)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como resultado debes de ver la salida de la red como un vector de dos elementos.\n",
    "\n",
    "Agrega tus conclusiones:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusión\n",
    "\n",
    "Esta práctica nos ha proporcionado una comprensión detallada y práctica de cómo funcionan las redes neuronales multicapa. Al construir la red desde cero, inicializando los pesos con distribuciones normales y calculando las salidas mediante la propagación hacia adelante, hemos adquirido una apreciación más profunda de los mecanismos subyacentes que hacen que las redes neuronales sean herramientas poderosas para el aprendizaje automático.\n",
    "\n",
    "Uno de los aprendizajes clave de esta práctica es la importancia de la vectorización en el cómputo de redes neuronales, permitiendo cálculos eficientes y escalables que son esenciales para trabajar con grandes volúmenes de datos. Además nos ha permitido construir una arquitectura de red neuronal desde cero, identificando con ello el numero de neruronas de entrada, ocultas y de salida, lo cual es crucial a la hora de construir un modelo de aprendizaje profundo, pues de no especificarlo de manera correcta existirán errores en la ejecución del código, o bien, caeremos en problemas de overfiting u underfiting\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nni",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "cbad788490f55b163920bee5e9d5e0cba00db5905dc94f4bdbe0011e55bf465f"
   }
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
