{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Descenso por gradiente\n",
    "\n",
    "En este notebook implementaremos un solo paso del método de descenso por gradiente. El método es una técnica de optimización utilizada para encontrar el mínimo de una función de manera iterativa. En el contexto de redes neuronales, se utiliza para minimizar la función de costo, que mide el error entre las predicciones del modelo y los valores reales. El proceso comienza con una estimación inicial para los parámetros del modelo, y luego, en cada paso, ajusta estos parámetros en la dirección opuesta al gradiente de la función de costo, que indica la dirección de mayor aumento. La magnitud del ajuste en cada paso se determina por un parámetro llamado tasa de aprendizaje. El proceso se repite hasta alcanzar un mínimo local o hasta que el cambio en la función de costo entre iteraciones sea insignificante, indicando que el modelo ha convergido a una solución.\n",
    "\n",
    "![gradiente](files/gradient_descent_1n_notebook.png)\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/irvingvasquez/cv2course_intro_nn/blob/master/03_descenso_unpaso.ipynb)\n",
    "\n",
    "@juan1rving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definimos la red neuronal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "# función de activación\n",
    "def sigmoid(h):\n",
    "    return 1/(1+np.exp(-h))\n",
    "\n",
    "# Derivada de f\n",
    "def sigmoid_prime(h):\n",
    "    return sigmoid(h) * (1 - sigmoid(h))\n",
    "\n",
    "# función h lineal\n",
    "def combinacion_lineal (X , W , b):\n",
    "    h = np.dot(W,X) + b\n",
    "    return h\n",
    "\n",
    "# Neurona\n",
    "def neurona(X,W,b):\n",
    "    return sigmoid(combinacion_lineal(X,W,b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Término de error\n",
    "\n",
    "Escribe una función que calcule el término de error\n",
    "\n",
    "$$\\delta= (y-\\hat{y})f' (h) = (y-\\hat{y})f' (\\sum_i w_i x_i)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "def error_term(y,W,X,b):\n",
    "    difference = y - neurona(X, W, b)\n",
    "    h = combinacion_lineal(X, W, b)\n",
    "    return difference * sigmoid_prime(h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Incremento\n",
    "\n",
    "Escribe una función para determinar el incremento a uno de los pesos\n",
    "$$\\Delta w_i= \\eta \\delta x_i$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "def increment(W, X, b, eta, i, y):\n",
    "    return eta * error_term(y, W, X, b) * X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_wights(W, increment_vector):\n",
    "    return W + increment_vector"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verificar funcionamiento\n",
    "\n",
    "A continuación implementemos una red de ejemplo y verificaremos que está funcionando almenos un paso del método de descenso por gradiente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "# valores de ejemplo\n",
    "learning_rate = 1.0\n",
    "X = np.array([1,1])\n",
    "y = 5\n",
    "\n",
    "# Valores iniciales de los pesos\n",
    "w = np.array([0.1,0.2])\n",
    "b = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\t***GRADIENT DESCENT METHOD***\n",
      "Inputs: [1 1]\n",
      "Objective value: 5\n",
      "Initial weights: [0.1 0.2]\n",
      "Learning rate: 1.0\n",
      "\n",
      "Output: 0.574442516811659\n",
      "Error: 4.425557483188341\n",
      "Increment vector: [1.08186431 1.08186431]\n",
      "New weights: [1.18186431 1.28186431]\n",
      "New error: 4.078440380493933\n",
      "\n",
      ">>>The Gradient Descent has been successful :)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\t\\t\\t***GRADIENT DESCENT METHOD***\")\n",
    "print(f\"Inputs: {X}\")\n",
    "print(f\"Objective value: {y}\")\n",
    "print(f\"Initial weights: {w}\")\n",
    "print(f\"Learning rate: {learning_rate}\\n\")\n",
    "\n",
    "salida = neurona(X, w, b)\n",
    "print('Output:', salida)\n",
    "\n",
    "residual = y - salida\n",
    "print('Error:', residual)\n",
    "\n",
    "# Calcula el incremento de los pesos\n",
    "incr_vector = increment(w, X, b, learning_rate, None, y)\n",
    "print('Increment vector:', incr_vector)\n",
    "\n",
    "# Calcula el nuevo valor del los pesos\n",
    "nw = update_wights(w, incr_vector)\n",
    "print('New weights:', nw)\n",
    "\n",
    "salida = neurona(X, nw, b)\n",
    "nuevo_error = y - salida\n",
    "print('New error:', nuevo_error)\n",
    "\n",
    "if nuevo_error < residual:\n",
    "    print(\"\\n>>>The Gradient Descent has been successful :)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusiones\n",
    "\n",
    "En esta práctica implementamos un paso del algoritmo de descenso por gradiente para una red neuronal simple. Utilizamos la función sigmoide como función de activación y calculamos el error en términos del gradiente de la función de error con respecto a los pesos de la red. A partir de esto, ajustamos los pesos para minimizar el error.\n",
    "\n",
    "Observamos que al actualizar los pesos con el incremento calculado, el error disminuyó, lo que indica un progreso en la dirección correcta hacia la minimización del error. Este proceso de ajuste iterativo de los pesos mediante el descenso por gradiente es fundamental en el entrenamiento de redes neuronales."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
