{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diseño de experimento\n",
    "\n",
    "El diseño de experimentos en el entrenamiento de redes neuronales juega un papel crucial en la optimización del rendimiento del modelo. Este enfoque sistemático permite explorar y ajustar los hiperparámetros (como la tasa de aprendizaje, el número de capas y neuronas, y el tamaño del lote) de manera eficiente y efectiva. Al planificar y estructurar los experimentos, se pueden identificar las combinaciones óptimas de hiperparámetros que mejoran la precisión y la generalización del modelo, mientras se minimiza el tiempo y los recursos computacionales necesarios. Además, un buen diseño de experimentos reduce el impacto ecológico del entrenamiento de redes neuronales al disminuir el número de ejecuciones necesarias, optimizando así el consumo de energía y hardware.\n",
    "\n",
    "## Una variable a la vez\n",
    "\n",
    "El enfoque de diseño de experimentos de una variable a la vez (One Variable At a Time, OVAT) es una metodología simple en la que se varía un solo factor o variable experimental mientras se mantienen constantes todos los demás factores. Este enfoque permite observar cómo cambios en esa única variable afectan el resultado del experimento. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Equipo:\n",
    "\n",
    "-Elias Nieto Víctor David\n",
    "\n",
    "-Pérez Lucio Kevyn Alejandro\n",
    "\n",
    "-Rojas Alarcon Sergio Ulises\n",
    "\n",
    "-Trejo Arriaga Rodrigo Gerardo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargamos paquetes necesarios\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Definición de variables y rango de valores:\n",
    "\n",
    "Identificar los factores o variables experimentales clave que se desean estudiar. En el contexto de redes neuronales, estas variables pueden incluir la tasa de aprendizaje, el número de capas, el número de neuronas por capa, el tamaño del lote, entre otros.\n",
    "\n",
    "Variables que tomaremos en cuenta:\n",
    "\n",
    "- Tasa de aprendizaje (eta)\n",
    "- Número de épocas (n_epocas)\n",
    "- Tamaño de lote (batch_size)\n",
    "\n",
    "Una vez definidas las variables independientes definimos un rango de valores posibles para cada variable.\n",
    "\n",
    "> TODO: Define un rango de valores posibles para cada variable. Incluye el valor mínimo y el valor máximo. Se sugiere utilizar una lista de valores obtenida con una separación uniforme. Probar almenos 5 valores por variable.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tasa de aprendizaje\n",
    "eta_values = [0.1, 0.01, 0.001, 0.0001, 0.00001]\n",
    "\n",
    "# Número de épocas\n",
    "n_epocas_values = [10, 15, 20, 25, 30]\n",
    "\n",
    "# Tamaño de lote\n",
    "batch_size_values = [16, 32, 64, 128, 256]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Configuración inicial:\n",
    "\n",
    "Establecer una configuración inicial para la red neuronal con valores predeterminados para todos los hiperparámetros. \n",
    "\n",
    "> TODO: Define la configuración inicial. Se sugiere usar un diccionario para contener dicha configuración.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "configuracion = {\n",
    "    'epochs': 10,\n",
    "    'batch_size': 64\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Variación de una variable a la vez:\n",
    "\n",
    "- Seleccionar la primera variable a estudiar (por ejemplo, la tasa de aprendizaje).\n",
    "- Realizar una serie de experimentos donde se varía únicamente la tasa de aprendizaje, mientras se mantienen constantes todos los demás hiperparámetros.\n",
    "- Registrar el rendimiento del modelo para cada valor de la tasa de aprendizaje.\n",
    "\n",
    "> TODO: Modifica el código para que pueda aceptar la configuración deseada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos una transformación de los datos\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "# Descargamos el conjunto de entrenamiento y cargamos mediante un dataLoader\n",
    "trainset = datasets.FashionMNIST('F_MNIST_data/', download=True, train=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "\n",
    "# Descargamos el conjunto de validación\n",
    "validationset = datasets.FashionMNIST('F_MNIST_data/', download=True, train=False, transform=transform)\n",
    "validationloader = torch.utils.data.DataLoader(validationset, batch_size=64, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definición del modelo de red neuronal\n",
    "class RedNeuronal(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_layers, drop_p=0.5):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Agregamos la primera capa\n",
    "        self.hidden_layers = nn.ModuleList([nn.Linear(input_size, hidden_layers[0])])\n",
    "        \n",
    "        # Agregamos cada una de las capas, zip empareja el número de entradas con las salidas\n",
    "        layer_sizes = zip(hidden_layers[:-1], hidden_layers[1:])\n",
    "        self.hidden_layers.extend([nn.Linear(h1, h2) for h1, h2 in layer_sizes])\n",
    "        \n",
    "        # Agregamos la capa de salida final de la red\n",
    "        self.output = nn.Linear(hidden_layers[-1], output_size)\n",
    "        \n",
    "        # Incluimos drop-out en la red\n",
    "        self.dropout = nn.Dropout(p=drop_p)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        ''' Pase hacia adelante en la red, el regreso son las probabilidades en el dominio log '''\n",
    "        for linear in self.hidden_layers:\n",
    "            x = F.relu(linear(x))\n",
    "            x = self.dropout(x)\n",
    "        \n",
    "        x = self.output(x)\n",
    "        \n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(model, validationloader, criterion):\n",
    "    test_loss = 0\n",
    "    accuracy = 0\n",
    "    for images, labels in validationloader:\n",
    "        images.resize_(images.shape[0], 784)\n",
    "        output = model.forward(images)\n",
    "        test_loss += criterion(output, labels).item()\n",
    "        ps = torch.exp(output)\n",
    "        equality = (labels.data == ps.max(dim=1)[1])\n",
    "        accuracy += equality.type(torch.FloatTensor).mean()\n",
    "    return test_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experimento 1: Evaluando tasa de aprendizaje 0.1\n",
      "Epoch: 1/10..  Tasa de aprendizaje: 0.1..  Pérdida de entrenamiento: 673.249..  Pérdida de validación: 2.818..  Exactitud de validación: 0.101\n",
      "Epoch: 2/10..  Tasa de aprendizaje: 0.1..  Pérdida de entrenamiento: 580.527..  Pérdida de validación: 2.624..  Exactitud de validación: 0.100\n",
      "Epoch: 3/10..  Tasa de aprendizaje: 0.1..  Pérdida de entrenamiento: 1127.660..  Pérdida de validación: 3.945..  Exactitud de validación: 0.100\n",
      "Epoch: 4/10..  Tasa de aprendizaje: 0.1..  Pérdida de entrenamiento: 1409.957..  Pérdida de validación: 7.246..  Exactitud de validación: 0.100\n",
      "Epoch: 5/10..  Tasa de aprendizaje: 0.1..  Pérdida de entrenamiento: 1452.069..  Pérdida de validación: 2.489..  Exactitud de validación: 0.100\n",
      "Epoch: 6/10..  Tasa de aprendizaje: 0.1..  Pérdida de entrenamiento: 1405.175..  Pérdida de validación: 2.332..  Exactitud de validación: 0.100\n",
      "Epoch: 7/10..  Tasa de aprendizaje: 0.1..  Pérdida de entrenamiento: 1543.589..  Pérdida de validación: 9.648..  Exactitud de validación: 0.100\n",
      "Epoch: 8/10..  Tasa de aprendizaje: 0.1..  Pérdida de entrenamiento: 1878.306..  Pérdida de validación: 4.151..  Exactitud de validación: 0.101\n",
      "Epoch: 9/10..  Tasa de aprendizaje: 0.1..  Pérdida de entrenamiento: 465.760..  Pérdida de validación: 2.310..  Exactitud de validación: 0.100\n",
      "Epoch: 10/10..  Tasa de aprendizaje: 0.1..  Pérdida de entrenamiento: 657.126..  Pérdida de validación: 2.308..  Exactitud de validación: 0.100\n",
      "Experimento 2: Evaluando tasa de aprendizaje 0.01\n",
      "Epoch: 1/10..  Tasa de aprendizaje: 0.01..  Pérdida de entrenamiento: 27.881..  Pérdida de validación: 0.810..  Exactitud de validación: 0.693\n",
      "Epoch: 2/10..  Tasa de aprendizaje: 0.01..  Pérdida de entrenamiento: 27.729..  Pérdida de validación: 0.837..  Exactitud de validación: 0.642\n",
      "Epoch: 3/10..  Tasa de aprendizaje: 0.01..  Pérdida de entrenamiento: 26.668..  Pérdida de validación: 0.736..  Exactitud de validación: 0.716\n",
      "Epoch: 4/10..  Tasa de aprendizaje: 0.01..  Pérdida de entrenamiento: 27.129..  Pérdida de validación: 0.777..  Exactitud de validación: 0.672\n",
      "Epoch: 5/10..  Tasa de aprendizaje: 0.01..  Pérdida de entrenamiento: 28.141..  Pérdida de validación: 0.731..  Exactitud de validación: 0.726\n",
      "Epoch: 6/10..  Tasa de aprendizaje: 0.01..  Pérdida de entrenamiento: 27.093..  Pérdida de validación: 0.710..  Exactitud de validación: 0.748\n",
      "Epoch: 7/10..  Tasa de aprendizaje: 0.01..  Pérdida de entrenamiento: 27.683..  Pérdida de validación: 0.935..  Exactitud de validación: 0.641\n",
      "Epoch: 8/10..  Tasa de aprendizaje: 0.01..  Pérdida de entrenamiento: 27.658..  Pérdida de validación: 0.812..  Exactitud de validación: 0.687\n",
      "Epoch: 9/10..  Tasa de aprendizaje: 0.01..  Pérdida de entrenamiento: 29.020..  Pérdida de validación: 1.056..  Exactitud de validación: 0.533\n",
      "Epoch: 10/10..  Tasa de aprendizaje: 0.01..  Pérdida de entrenamiento: 28.056..  Pérdida de validación: 0.876..  Exactitud de validación: 0.659\n",
      "Experimento 3: Evaluando tasa de aprendizaje 0.001\n",
      "Epoch: 1/10..  Tasa de aprendizaje: 0.001..  Pérdida de entrenamiento: 14.401..  Pérdida de validación: 0.456..  Exactitud de validación: 0.833\n",
      "Epoch: 2/10..  Tasa de aprendizaje: 0.001..  Pérdida de entrenamiento: 11.272..  Pérdida de validación: 0.415..  Exactitud de validación: 0.847\n",
      "Epoch: 3/10..  Tasa de aprendizaje: 0.001..  Pérdida de entrenamiento: 10.589..  Pérdida de validación: 0.397..  Exactitud de validación: 0.854\n",
      "Epoch: 4/10..  Tasa de aprendizaje: 0.001..  Pérdida de entrenamiento: 10.224..  Pérdida de validación: 0.390..  Exactitud de validación: 0.857\n",
      "Epoch: 5/10..  Tasa de aprendizaje: 0.001..  Pérdida de entrenamiento: 10.015..  Pérdida de validación: 0.392..  Exactitud de validación: 0.858\n",
      "Epoch: 6/10..  Tasa de aprendizaje: 0.001..  Pérdida de entrenamiento: 9.651..  Pérdida de validación: 0.378..  Exactitud de validación: 0.864\n",
      "Epoch: 7/10..  Tasa de aprendizaje: 0.001..  Pérdida de entrenamiento: 9.516..  Pérdida de validación: 0.378..  Exactitud de validación: 0.861\n",
      "Epoch: 8/10..  Tasa de aprendizaje: 0.001..  Pérdida de entrenamiento: 9.272..  Pérdida de validación: 0.371..  Exactitud de validación: 0.866\n",
      "Epoch: 9/10..  Tasa de aprendizaje: 0.001..  Pérdida de entrenamiento: 9.059..  Pérdida de validación: 0.372..  Exactitud de validación: 0.863\n",
      "Epoch: 10/10..  Tasa de aprendizaje: 0.001..  Pérdida de entrenamiento: 8.941..  Pérdida de validación: 0.358..  Exactitud de validación: 0.868\n",
      "Experimento 4: Evaluando tasa de aprendizaje 0.0001\n",
      "Epoch: 1/10..  Tasa de aprendizaje: 0.0001..  Pérdida de entrenamiento: 18.755..  Pérdida de validación: 0.520..  Exactitud de validación: 0.808\n",
      "Epoch: 2/10..  Tasa de aprendizaje: 0.0001..  Pérdida de entrenamiento: 12.104..  Pérdida de validación: 0.458..  Exactitud de validación: 0.832\n",
      "Epoch: 3/10..  Tasa de aprendizaje: 0.0001..  Pérdida de entrenamiento: 10.789..  Pérdida de validación: 0.424..  Exactitud de validación: 0.846\n",
      "Epoch: 4/10..  Tasa de aprendizaje: 0.0001..  Pérdida de entrenamiento: 10.063..  Pérdida de validación: 0.407..  Exactitud de validación: 0.852\n",
      "Epoch: 5/10..  Tasa de aprendizaje: 0.0001..  Pérdida de entrenamiento: 9.545..  Pérdida de validación: 0.398..  Exactitud de validación: 0.855\n",
      "Epoch: 6/10..  Tasa de aprendizaje: 0.0001..  Pérdida de entrenamiento: 9.100..  Pérdida de validación: 0.385..  Exactitud de validación: 0.860\n",
      "Epoch: 7/10..  Tasa de aprendizaje: 0.0001..  Pérdida de entrenamiento: 8.809..  Pérdida de validación: 0.371..  Exactitud de validación: 0.864\n",
      "Epoch: 8/10..  Tasa de aprendizaje: 0.0001..  Pérdida de entrenamiento: 8.544..  Pérdida de validación: 0.376..  Exactitud de validación: 0.865\n",
      "Epoch: 9/10..  Tasa de aprendizaje: 0.0001..  Pérdida de entrenamiento: 8.283..  Pérdida de validación: 0.359..  Exactitud de validación: 0.867\n",
      "Epoch: 10/10..  Tasa de aprendizaje: 0.0001..  Pérdida de entrenamiento: 8.075..  Pérdida de validación: 0.356..  Exactitud de validación: 0.870\n",
      "Experimento 5: Evaluando tasa de aprendizaje 1e-05\n",
      "Epoch: 1/10..  Tasa de aprendizaje: 1e-05..  Pérdida de entrenamiento: 38.791..  Pérdida de validación: 0.984..  Exactitud de validación: 0.716\n",
      "Epoch: 2/10..  Tasa de aprendizaje: 1e-05..  Pérdida de entrenamiento: 22.375..  Pérdida de validación: 0.731..  Exactitud de validación: 0.745\n",
      "Epoch: 3/10..  Tasa de aprendizaje: 1e-05..  Pérdida de entrenamiento: 18.599..  Pérdida de validación: 0.654..  Exactitud de validación: 0.762\n",
      "Epoch: 4/10..  Tasa de aprendizaje: 1e-05..  Pérdida de entrenamiento: 16.858..  Pérdida de validación: 0.613..  Exactitud de validación: 0.774\n",
      "Epoch: 5/10..  Tasa de aprendizaje: 1e-05..  Pérdida de entrenamiento: 15.704..  Pérdida de validación: 0.583..  Exactitud de validación: 0.785\n",
      "Epoch: 6/10..  Tasa de aprendizaje: 1e-05..  Pérdida de entrenamiento: 14.872..  Pérdida de validación: 0.558..  Exactitud de validación: 0.795\n",
      "Epoch: 7/10..  Tasa de aprendizaje: 1e-05..  Pérdida de entrenamiento: 14.169..  Pérdida de validación: 0.538..  Exactitud de validación: 0.802\n",
      "Epoch: 8/10..  Tasa de aprendizaje: 1e-05..  Pérdida de entrenamiento: 13.570..  Pérdida de validación: 0.522..  Exactitud de validación: 0.810\n",
      "Epoch: 9/10..  Tasa de aprendizaje: 1e-05..  Pérdida de entrenamiento: 13.161..  Pérdida de validación: 0.511..  Exactitud de validación: 0.814\n",
      "Epoch: 10/10..  Tasa de aprendizaje: 1e-05..  Pérdida de entrenamiento: 12.799..  Pérdida de validación: 0.499..  Exactitud de validación: 0.818\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "\n",
    "for i, eta in enumerate(eta_values):\n",
    "    print(f\"Experimento {i+1}: Evaluando tasa de aprendizaje {eta}\")\n",
    "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=configuracion['batch_size'], shuffle=True)\n",
    "    validationloader = torch.utils.data.DataLoader(validationset, batch_size=configuracion['batch_size'], shuffle=True)\n",
    "    \n",
    "    model = RedNeuronal(784, 10, [516, 256], drop_p=0.5)\n",
    "    criterion = nn.NLLLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=eta)\n",
    "    epochs = configuracion['epochs']\n",
    "    steps = 0\n",
    "    running_loss = 0\n",
    "    print_every = 40\n",
    "    \n",
    "    for e in range(epochs):\n",
    "        model.train()\n",
    "        for images, labels in trainloader:\n",
    "            steps += 1\n",
    "            images.resize_(images.size()[0], 784)\n",
    "            optimizer.zero_grad()\n",
    "            output = model.forward(images)\n",
    "            loss = criterion(output, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            test_loss, accuracy = validation(model, validationloader, criterion)\n",
    "        print(f\"Epoch: {e+1}/{epochs}.. \",\n",
    "              f\"Tasa de aprendizaje: {eta}.. \",\n",
    "              f\"Pérdida de entrenamiento: {running_loss/print_every:.3f}.. \",\n",
    "              f\"Pérdida de validación: {test_loss/len(validationloader):.3f}.. \",\n",
    "              f\"Exactitud de validación: {accuracy/len(validationloader):.3f}\")\n",
    "        results.append([eta, epochs, configuracion['batch_size'], running_loss/print_every, test_loss/len(validationloader), accuracy/len(validationloader)])\n",
    "        running_loss = 0\n",
    "        model.train()\n",
    "\n",
    "df_results = pd.DataFrame(results, columns=['Learning Rate', 'Epochs', 'Batch Size', 'Training Loss', 'Validation Loss', 'Validation Accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Selección del mejor valor:\n",
    "\n",
    "Analizar los resultados y seleccionar el valor de la tasa de aprendizaje que produce el mejor rendimiento del modelo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Si se ejecuta por primera vez, descomentar la linea siguiente\n",
    "# df_results['Validation Accuracy'] = df_results['Validation Accuracy'].apply(lambda x: x.item())\n",
    "last_epoch_results = df_results.groupby(['Learning Rate']).tail(1).reset_index(drop=True)\n",
    "best_result = last_epoch_results.loc[last_epoch_results['Validation Accuracy'].idxmax()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Learning Rate</th>\n",
       "      <th>Epochs</th>\n",
       "      <th>Batch Size</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Validation Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.10000</td>\n",
       "      <td>10</td>\n",
       "      <td>64</td>\n",
       "      <td>657.125797</td>\n",
       "      <td>2.307695</td>\n",
       "      <td>0.099622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.01000</td>\n",
       "      <td>10</td>\n",
       "      <td>64</td>\n",
       "      <td>28.055921</td>\n",
       "      <td>0.876105</td>\n",
       "      <td>0.658838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.00100</td>\n",
       "      <td>10</td>\n",
       "      <td>64</td>\n",
       "      <td>8.940564</td>\n",
       "      <td>0.357506</td>\n",
       "      <td>0.868332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.00010</td>\n",
       "      <td>10</td>\n",
       "      <td>64</td>\n",
       "      <td>8.074979</td>\n",
       "      <td>0.355960</td>\n",
       "      <td>0.869825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.00001</td>\n",
       "      <td>10</td>\n",
       "      <td>64</td>\n",
       "      <td>12.799471</td>\n",
       "      <td>0.499012</td>\n",
       "      <td>0.818073</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Learning Rate  Epochs  Batch Size  Training Loss  Validation Loss  \\\n",
       "0        0.10000      10          64     657.125797         2.307695   \n",
       "1        0.01000      10          64      28.055921         0.876105   \n",
       "2        0.00100      10          64       8.940564         0.357506   \n",
       "3        0.00010      10          64       8.074979         0.355960   \n",
       "4        0.00001      10          64      12.799471         0.499012   \n",
       "\n",
       "   Validation Accuracy  \n",
       "0             0.099622  \n",
       "1             0.658838  \n",
       "2             0.868332  \n",
       "3             0.869825  \n",
       "4             0.818073  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_epoch_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mejor tasa de aprendizaje:\n",
      "0.0001\n",
      "\n",
      "Resultados asociados al mejor valor:\n",
      "Learning Rate           0.000100\n",
      "Epochs                 10.000000\n",
      "Batch Size             64.000000\n",
      "Training Loss           8.074979\n",
      "Validation Loss         0.355960\n",
      "Validation Accuracy     0.869825\n",
      "Name: 3, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nMejor tasa de aprendizaje:\")\n",
    "print(best_result['Learning Rate'])\n",
    "print(\"\\nResultados asociados al mejor valor:\")\n",
    "print(best_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Repetición para otras variables:\n",
    "\n",
    "Proceder con la siguiente variable (por ejemplo, el número de capas) y repetir el proceso: variar solo esta variable mientras se mantienen constantes todos los demás hiperparámetros, utilizando el mejor valor encontrado para la tasa de aprendizaje. Continuar este proceso para cada variable en la lista.\n",
    "\n",
    "> TODO: Escribe una tabla con el resultado de cada experimento. Las columnas deben ser: ID, Configuración, Exactitud obtenida."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fijamos el learning rate en 0.0001 y variamos el tamaño del batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experimento 1: Evaluando tamaño de lote 16\n",
      "Epoch: 1/10..  Tamaño de lote: 16..  Pérdida de entrenamiento: 63.151..  Pérdida de validación: 0.463..  Exactitud de validación: 0.830\n",
      "Epoch: 2/10..  Tamaño de lote: 16..  Pérdida de entrenamiento: 43.787..  Pérdida de validación: 0.417..  Exactitud de validación: 0.846\n",
      "Epoch: 3/10..  Tamaño de lote: 16..  Pérdida de entrenamiento: 39.576..  Pérdida de validación: 0.402..  Exactitud de validación: 0.853\n",
      "Epoch: 4/10..  Tamaño de lote: 16..  Pérdida de entrenamiento: 37.214..  Pérdida de validación: 0.382..  Exactitud de validación: 0.859\n",
      "Epoch: 5/10..  Tamaño de lote: 16..  Pérdida de entrenamiento: 35.398..  Pérdida de validación: 0.370..  Exactitud de validación: 0.865\n",
      "Epoch: 6/10..  Tamaño de lote: 16..  Pérdida de entrenamiento: 34.101..  Pérdida de validación: 0.363..  Exactitud de validación: 0.871\n",
      "Epoch: 7/10..  Tamaño de lote: 16..  Pérdida de entrenamiento: 33.056..  Pérdida de validación: 0.354..  Exactitud de validación: 0.873\n",
      "Epoch: 8/10..  Tamaño de lote: 16..  Pérdida de entrenamiento: 31.928..  Pérdida de validación: 0.344..  Exactitud de validación: 0.877\n",
      "Epoch: 9/10..  Tamaño de lote: 16..  Pérdida de entrenamiento: 31.222..  Pérdida de validación: 0.336..  Exactitud de validación: 0.881\n",
      "Epoch: 10/10..  Tamaño de lote: 16..  Pérdida de entrenamiento: 30.526..  Pérdida de validación: 0.344..  Exactitud de validación: 0.874\n",
      "Experimento 2: Evaluando tamaño de lote 32\n",
      "Epoch: 1/10..  Tamaño de lote: 32..  Pérdida de entrenamiento: 34.179..  Pérdida de validación: 0.488..  Exactitud de validación: 0.822\n",
      "Epoch: 2/10..  Tamaño de lote: 32..  Pérdida de entrenamiento: 23.015..  Pérdida de validación: 0.436..  Exactitud de validación: 0.841\n",
      "Epoch: 3/10..  Tamaño de lote: 32..  Pérdida de entrenamiento: 20.559..  Pérdida de validación: 0.423..  Exactitud de validación: 0.846\n",
      "Epoch: 4/10..  Tamaño de lote: 32..  Pérdida de entrenamiento: 19.253..  Pérdida de validación: 0.391..  Exactitud de validación: 0.858\n",
      "Epoch: 5/10..  Tamaño de lote: 32..  Pérdida de entrenamiento: 18.247..  Pérdida de validación: 0.380..  Exactitud de validación: 0.861\n",
      "Epoch: 6/10..  Tamaño de lote: 32..  Pérdida de entrenamiento: 17.494..  Pérdida de validación: 0.374..  Exactitud de validación: 0.865\n",
      "Epoch: 7/10..  Tamaño de lote: 32..  Pérdida de entrenamiento: 16.847..  Pérdida de validación: 0.361..  Exactitud de validación: 0.868\n",
      "Epoch: 8/10..  Tamaño de lote: 32..  Pérdida de entrenamiento: 16.424..  Pérdida de validación: 0.352..  Exactitud de validación: 0.874\n",
      "Epoch: 9/10..  Tamaño de lote: 32..  Pérdida de entrenamiento: 15.873..  Pérdida de validación: 0.346..  Exactitud de validación: 0.873\n",
      "Epoch: 10/10..  Tamaño de lote: 32..  Pérdida de entrenamiento: 15.494..  Pérdida de validación: 0.347..  Exactitud de validación: 0.877\n",
      "Experimento 3: Evaluando tamaño de lote 64\n",
      "Epoch: 1/10..  Tamaño de lote: 64..  Pérdida de entrenamiento: 18.915..  Pérdida de validación: 0.521..  Exactitud de validación: 0.808\n",
      "Epoch: 2/10..  Tamaño de lote: 64..  Pérdida de entrenamiento: 12.187..  Pérdida de validación: 0.461..  Exactitud de validación: 0.833\n",
      "Epoch: 3/10..  Tamaño de lote: 64..  Pérdida de entrenamiento: 10.819..  Pérdida de validación: 0.430..  Exactitud de validación: 0.840\n",
      "Epoch: 4/10..  Tamaño de lote: 64..  Pérdida de entrenamiento: 10.085..  Pérdida de validación: 0.410..  Exactitud de validación: 0.851\n",
      "Epoch: 5/10..  Tamaño de lote: 64..  Pérdida de entrenamiento: 9.543..  Pérdida de validación: 0.391..  Exactitud de validación: 0.858\n",
      "Epoch: 6/10..  Tamaño de lote: 64..  Pérdida de entrenamiento: 9.160..  Pérdida de validación: 0.386..  Exactitud de validación: 0.860\n",
      "Epoch: 7/10..  Tamaño de lote: 64..  Pérdida de entrenamiento: 8.841..  Pérdida de validación: 0.378..  Exactitud de validación: 0.862\n",
      "Epoch: 8/10..  Tamaño de lote: 64..  Pérdida de entrenamiento: 8.537..  Pérdida de validación: 0.367..  Exactitud de validación: 0.866\n",
      "Epoch: 9/10..  Tamaño de lote: 64..  Pérdida de entrenamiento: 8.297..  Pérdida de validación: 0.360..  Exactitud de validación: 0.871\n",
      "Epoch: 10/10..  Tamaño de lote: 64..  Pérdida de entrenamiento: 8.065..  Pérdida de validación: 0.352..  Exactitud de validación: 0.872\n",
      "Experimento 4: Evaluando tamaño de lote 128\n",
      "Epoch: 1/10..  Tamaño de lote: 128..  Pérdida de entrenamiento: 10.739..  Pérdida de validación: 0.558..  Exactitud de validación: 0.796\n",
      "Epoch: 2/10..  Tamaño de lote: 128..  Pérdida de entrenamiento: 6.565..  Pérdida de validación: 0.483..  Exactitud de validación: 0.823\n",
      "Epoch: 3/10..  Tamaño de lote: 128..  Pérdida de entrenamiento: 5.759..  Pérdida de validación: 0.445..  Exactitud de validación: 0.836\n",
      "Epoch: 4/10..  Tamaño de lote: 128..  Pérdida de entrenamiento: 5.374..  Pérdida de validación: 0.427..  Exactitud de validación: 0.844\n",
      "Epoch: 5/10..  Tamaño de lote: 128..  Pérdida de entrenamiento: 5.042..  Pérdida de validación: 0.415..  Exactitud de validación: 0.848\n",
      "Epoch: 6/10..  Tamaño de lote: 128..  Pérdida de entrenamiento: 4.821..  Pérdida de validación: 0.395..  Exactitud de validación: 0.856\n",
      "Epoch: 7/10..  Tamaño de lote: 128..  Pérdida de entrenamiento: 4.650..  Pérdida de validación: 0.384..  Exactitud de validación: 0.860\n",
      "Epoch: 8/10..  Tamaño de lote: 128..  Pérdida de entrenamiento: 4.539..  Pérdida de validación: 0.376..  Exactitud de validación: 0.864\n",
      "Epoch: 9/10..  Tamaño de lote: 128..  Pérdida de entrenamiento: 4.379..  Pérdida de validación: 0.376..  Exactitud de validación: 0.863\n",
      "Epoch: 10/10..  Tamaño de lote: 128..  Pérdida de entrenamiento: 4.265..  Pérdida de validación: 0.367..  Exactitud de validación: 0.868\n",
      "Experimento 5: Evaluando tamaño de lote 256\n",
      "Epoch: 1/10..  Tamaño de lote: 256..  Pérdida de entrenamiento: 6.277..  Pérdida de validación: 0.604..  Exactitud de validación: 0.779\n",
      "Epoch: 2/10..  Tamaño de lote: 256..  Pérdida de entrenamiento: 3.647..  Pérdida de validación: 0.521..  Exactitud de validación: 0.805\n",
      "Epoch: 3/10..  Tamaño de lote: 256..  Pérdida de entrenamiento: 3.163..  Pérdida de validación: 0.478..  Exactitud de validación: 0.824\n",
      "Epoch: 4/10..  Tamaño de lote: 256..  Pérdida de entrenamiento: 2.914..  Pérdida de validación: 0.456..  Exactitud de validación: 0.834\n",
      "Epoch: 5/10..  Tamaño de lote: 256..  Pérdida de entrenamiento: 2.744..  Pérdida de validación: 0.436..  Exactitud de validación: 0.842\n",
      "Epoch: 6/10..  Tamaño de lote: 256..  Pérdida de entrenamiento: 2.615..  Pérdida de validación: 0.424..  Exactitud de validación: 0.842\n",
      "Epoch: 7/10..  Tamaño de lote: 256..  Pérdida de entrenamiento: 2.515..  Pérdida de validación: 0.420..  Exactitud de validación: 0.844\n",
      "Epoch: 8/10..  Tamaño de lote: 256..  Pérdida de entrenamiento: 2.435..  Pérdida de validación: 0.417..  Exactitud de validación: 0.849\n",
      "Epoch: 9/10..  Tamaño de lote: 256..  Pérdida de entrenamiento: 2.362..  Pérdida de validación: 0.402..  Exactitud de validación: 0.855\n",
      "Epoch: 10/10..  Tamaño de lote: 256..  Pérdida de entrenamiento: 2.307..  Pérdida de validación: 0.384..  Exactitud de validación: 0.860\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.0001\n",
    "epochs = 10\n",
    "results = []\n",
    "\n",
    "for i, batch_size in enumerate(batch_size_values):\n",
    "    print(f\"Experimento {i+1}: Evaluando tamaño de lote {batch_size}\")\n",
    "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
    "    validationloader = torch.utils.data.DataLoader(validationset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    model = RedNeuronal(784, 10, [516, 256], drop_p=0.5)\n",
    "    criterion = nn.NLLLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    steps = 0\n",
    "    running_loss = 0\n",
    "    print_every = 40\n",
    "    \n",
    "    for e in range(epochs):\n",
    "        model.train()\n",
    "        for images, labels in trainloader:\n",
    "            steps += 1\n",
    "            images.resize_(images.size()[0], 784)\n",
    "            optimizer.zero_grad()\n",
    "            output = model.forward(images)\n",
    "            loss = criterion(output, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            test_loss, accuracy = validation(model, validationloader, criterion)\n",
    "        print(f\"Epoch: {e+1}/{epochs}.. \",\n",
    "              f\"Tamaño de lote: {batch_size}.. \",\n",
    "              f\"Pérdida de entrenamiento: {running_loss/print_every:.3f}.. \",\n",
    "              f\"Pérdida de validación: {test_loss/len(validationloader):.3f}.. \",\n",
    "              f\"Exactitud de validación: {accuracy/len(validationloader):.3f}\")\n",
    "        results.append([learning_rate, epochs, batch_size, running_loss/print_every, test_loss/len(validationloader), accuracy/len(validationloader)])\n",
    "        running_loss = 0\n",
    "        model.train()\n",
    "\n",
    "df_batch_results = pd.DataFrame(results, columns=['Learning Rate', 'Epochs', 'Batch Size', 'Training Loss', 'Validation Loss', 'Validation Accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_last_epoch = df_results[df_results['Epochs'] == epochs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_result = df_last_epoch.loc[df_last_epoch['Validation Accuracy'].idxmax()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Learning Rate           0.000100\n",
       "Epochs                 10.000000\n",
       "Batch Size             64.000000\n",
       "Training Loss           8.074979\n",
       "Validation Loss         0.355960\n",
       "Validation Accuracy     0.869825\n",
       "Name: 39, dtype: float64"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mejor tasa de aprendizaje:\n",
      "64.0\n",
      "\n",
      "Resultados asociados al mejor valor:\n",
      "Learning Rate           0.000100\n",
      "Epochs                 10.000000\n",
      "Batch Size             64.000000\n",
      "Training Loss           8.074979\n",
      "Validation Loss         0.355960\n",
      "Validation Accuracy     0.869825\n",
      "Name: 39, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nMejor tasa de aprendizaje:\")\n",
    "print(best_result['Batch Size'])\n",
    "print(\"\\nResultados asociados al mejor valor:\")\n",
    "print(best_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fijamos el learning rate en 0.0001 y el tamaño del batch en 64 mientras variamos el numero de épocas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experimento 1: Evaluando número de épocas 5\n",
      "Epoch: 1/5..  Número de épocas: 5..  Pérdida de entrenamiento: 1.066..  Pérdida de validación: 0.612..  Exactitud de validación: 0.777\n",
      "Epoch: 2/5..  Número de épocas: 5..  Pérdida de entrenamiento: 0.619..  Pérdida de validación: 0.524..  Exactitud de validación: 0.806\n",
      "Epoch: 3/5..  Número de épocas: 5..  Pérdida de entrenamiento: 0.538..  Pérdida de validación: 0.473..  Exactitud de validación: 0.825\n",
      "Epoch: 4/5..  Número de épocas: 5..  Pérdida de entrenamiento: 0.495..  Pérdida de validación: 0.458..  Exactitud de validación: 0.831\n",
      "Epoch: 5/5..  Número de épocas: 5..  Pérdida de entrenamiento: 0.464..  Pérdida de validación: 0.443..  Exactitud de validación: 0.840\n",
      "Experimento 2: Evaluando número de épocas 10\n",
      "Epoch: 1/10..  Número de épocas: 10..  Pérdida de entrenamiento: 1.091..  Pérdida de validación: 0.619..  Exactitud de validación: 0.775\n",
      "Epoch: 2/10..  Número de épocas: 10..  Pérdida de entrenamiento: 0.629..  Pérdida de validación: 0.521..  Exactitud de validación: 0.813\n",
      "Epoch: 3/10..  Número de épocas: 10..  Pérdida de entrenamiento: 0.542..  Pérdida de validación: 0.489..  Exactitud de validación: 0.818\n",
      "Epoch: 4/10..  Número de épocas: 10..  Pérdida de entrenamiento: 0.498..  Pérdida de validación: 0.457..  Exactitud de validación: 0.834\n",
      "Epoch: 5/10..  Número de épocas: 10..  Pérdida de entrenamiento: 0.470..  Pérdida de validación: 0.436..  Exactitud de validación: 0.839\n",
      "Epoch: 6/10..  Número de épocas: 10..  Pérdida de entrenamiento: 0.447..  Pérdida de validación: 0.430..  Exactitud de validación: 0.844\n",
      "Epoch: 7/10..  Número de épocas: 10..  Pérdida de entrenamiento: 0.430..  Pérdida de validación: 0.412..  Exactitud de validación: 0.848\n",
      "Epoch: 8/10..  Número de épocas: 10..  Pérdida de entrenamiento: 0.414..  Pérdida de validación: 0.419..  Exactitud de validación: 0.848\n",
      "Epoch: 9/10..  Número de épocas: 10..  Pérdida de entrenamiento: 0.404..  Pérdida de validación: 0.391..  Exactitud de validación: 0.860\n",
      "Epoch: 10/10..  Número de épocas: 10..  Pérdida de entrenamiento: 0.391..  Pérdida de validación: 0.393..  Exactitud de validación: 0.860\n",
      "Experimento 3: Evaluando número de épocas 15\n",
      "Epoch: 1/15..  Número de épocas: 15..  Pérdida de entrenamiento: 1.068..  Pérdida de validación: 0.609..  Exactitud de validación: 0.774\n",
      "Epoch: 2/15..  Número de épocas: 15..  Pérdida de entrenamiento: 0.616..  Pérdida de validación: 0.525..  Exactitud de validación: 0.808\n",
      "Epoch: 3/15..  Número de épocas: 15..  Pérdida de entrenamiento: 0.538..  Pérdida de validación: 0.480..  Exactitud de validación: 0.823\n",
      "Epoch: 4/15..  Número de épocas: 15..  Pérdida de entrenamiento: 0.497..  Pérdida de validación: 0.454..  Exactitud de validación: 0.834\n",
      "Epoch: 5/15..  Número de épocas: 15..  Pérdida de entrenamiento: 0.466..  Pérdida de validación: 0.437..  Exactitud de validación: 0.838\n",
      "Epoch: 6/15..  Número de épocas: 15..  Pérdida de entrenamiento: 0.445..  Pérdida de validación: 0.422..  Exactitud de validación: 0.845\n",
      "Epoch: 7/15..  Número de épocas: 15..  Pérdida de entrenamiento: 0.428..  Pérdida de validación: 0.414..  Exactitud de validación: 0.845\n",
      "Epoch: 8/15..  Número de épocas: 15..  Pérdida de entrenamiento: 0.414..  Pérdida de validación: 0.411..  Exactitud de validación: 0.851\n",
      "Epoch: 9/15..  Número de épocas: 15..  Pérdida de entrenamiento: 0.402..  Pérdida de validación: 0.391..  Exactitud de validación: 0.855\n",
      "Epoch: 10/15..  Número de épocas: 15..  Pérdida de entrenamiento: 0.391..  Pérdida de validación: 0.385..  Exactitud de validación: 0.857\n",
      "Epoch: 11/15..  Número de épocas: 15..  Pérdida de entrenamiento: 0.383..  Pérdida de validación: 0.383..  Exactitud de validación: 0.859\n",
      "Epoch: 12/15..  Número de épocas: 15..  Pérdida de entrenamiento: 0.374..  Pérdida de validación: 0.382..  Exactitud de validación: 0.864\n",
      "Epoch: 13/15..  Número de épocas: 15..  Pérdida de entrenamiento: 0.367..  Pérdida de validación: 0.375..  Exactitud de validación: 0.864\n",
      "Epoch: 14/15..  Número de épocas: 15..  Pérdida de entrenamiento: 0.358..  Pérdida de validación: 0.363..  Exactitud de validación: 0.869\n",
      "Epoch: 15/15..  Número de épocas: 15..  Pérdida de entrenamiento: 0.354..  Pérdida de validación: 0.362..  Exactitud de validación: 0.870\n",
      "Experimento 4: Evaluando número de épocas 20\n",
      "Epoch: 1/20..  Número de épocas: 20..  Pérdida de entrenamiento: 1.068..  Pérdida de validación: 0.613..  Exactitud de validación: 0.767\n",
      "Epoch: 2/20..  Número de épocas: 20..  Pérdida de entrenamiento: 0.620..  Pérdida de validación: 0.522..  Exactitud de validación: 0.808\n",
      "Epoch: 3/20..  Número de épocas: 20..  Pérdida de entrenamiento: 0.539..  Pérdida de validación: 0.482..  Exactitud de validación: 0.824\n",
      "Epoch: 4/20..  Número de épocas: 20..  Pérdida de entrenamiento: 0.498..  Pérdida de validación: 0.455..  Exactitud de validación: 0.835\n",
      "Epoch: 5/20..  Número de épocas: 20..  Pérdida de entrenamiento: 0.468..  Pérdida de validación: 0.437..  Exactitud de validación: 0.840\n",
      "Epoch: 6/20..  Número de épocas: 20..  Pérdida de entrenamiento: 0.447..  Pérdida de validación: 0.425..  Exactitud de validación: 0.843\n",
      "Epoch: 7/20..  Número de épocas: 20..  Pérdida de entrenamiento: 0.430..  Pérdida de validación: 0.410..  Exactitud de validación: 0.850\n",
      "Epoch: 8/20..  Número de épocas: 20..  Pérdida de entrenamiento: 0.415..  Pérdida de validación: 0.399..  Exactitud de validación: 0.853\n",
      "Epoch: 9/20..  Número de épocas: 20..  Pérdida de entrenamiento: 0.403..  Pérdida de validación: 0.393..  Exactitud de validación: 0.856\n",
      "Epoch: 10/20..  Número de épocas: 20..  Pérdida de entrenamiento: 0.393..  Pérdida de validación: 0.384..  Exactitud de validación: 0.860\n",
      "Epoch: 11/20..  Número de épocas: 20..  Pérdida de entrenamiento: 0.383..  Pérdida de validación: 0.378..  Exactitud de validación: 0.863\n",
      "Epoch: 12/20..  Número de épocas: 20..  Pérdida de entrenamiento: 0.376..  Pérdida de validación: 0.378..  Exactitud de validación: 0.860\n",
      "Epoch: 13/20..  Número de épocas: 20..  Pérdida de entrenamiento: 0.369..  Pérdida de validación: 0.371..  Exactitud de validación: 0.863\n",
      "Epoch: 14/20..  Número de épocas: 20..  Pérdida de entrenamiento: 0.363..  Pérdida de validación: 0.361..  Exactitud de validación: 0.870\n",
      "Epoch: 15/20..  Número de épocas: 20..  Pérdida de entrenamiento: 0.353..  Pérdida de validación: 0.358..  Exactitud de validación: 0.872\n",
      "Epoch: 16/20..  Número de épocas: 20..  Pérdida de entrenamiento: 0.349..  Pérdida de validación: 0.362..  Exactitud de validación: 0.870\n",
      "Epoch: 17/20..  Número de épocas: 20..  Pérdida de entrenamiento: 0.341..  Pérdida de validación: 0.352..  Exactitud de validación: 0.873\n",
      "Epoch: 18/20..  Número de épocas: 20..  Pérdida de entrenamiento: 0.338..  Pérdida de validación: 0.350..  Exactitud de validación: 0.873\n",
      "Epoch: 19/20..  Número de épocas: 20..  Pérdida de entrenamiento: 0.331..  Pérdida de validación: 0.346..  Exactitud de validación: 0.875\n",
      "Epoch: 20/20..  Número de épocas: 20..  Pérdida de entrenamiento: 0.327..  Pérdida de validación: 0.347..  Exactitud de validación: 0.875\n",
      "Experimento 5: Evaluando número de épocas 25\n",
      "Epoch: 1/25..  Número de épocas: 25..  Pérdida de entrenamiento: 1.070..  Pérdida de validación: 0.608..  Exactitud de validación: 0.774\n",
      "Epoch: 2/25..  Número de épocas: 25..  Pérdida de entrenamiento: 0.619..  Pérdida de validación: 0.523..  Exactitud de validación: 0.807\n",
      "Epoch: 3/25..  Número de épocas: 25..  Pérdida de entrenamiento: 0.539..  Pérdida de validación: 0.476..  Exactitud de validación: 0.826\n",
      "Epoch: 4/25..  Número de épocas: 25..  Pérdida de entrenamiento: 0.495..  Pérdida de validación: 0.458..  Exactitud de validación: 0.831\n",
      "Epoch: 5/25..  Número de épocas: 25..  Pérdida de entrenamiento: 0.465..  Pérdida de validación: 0.437..  Exactitud de validación: 0.838\n",
      "Epoch: 6/25..  Número de épocas: 25..  Pérdida de entrenamiento: 0.445..  Pérdida de validación: 0.421..  Exactitud de validación: 0.844\n",
      "Epoch: 7/25..  Número de épocas: 25..  Pérdida de entrenamiento: 0.428..  Pérdida de validación: 0.425..  Exactitud de validación: 0.846\n",
      "Epoch: 8/25..  Número de épocas: 25..  Pérdida de entrenamiento: 0.415..  Pérdida de validación: 0.407..  Exactitud de validación: 0.850\n",
      "Epoch: 9/25..  Número de épocas: 25..  Pérdida de entrenamiento: 0.402..  Pérdida de validación: 0.389..  Exactitud de validación: 0.859\n",
      "Epoch: 10/25..  Número de épocas: 25..  Pérdida de entrenamiento: 0.392..  Pérdida de validación: 0.389..  Exactitud de validación: 0.862\n",
      "Epoch: 11/25..  Número de épocas: 25..  Pérdida de entrenamiento: 0.382..  Pérdida de validación: 0.386..  Exactitud de validación: 0.860\n",
      "Epoch: 12/25..  Número de épocas: 25..  Pérdida de entrenamiento: 0.374..  Pérdida de validación: 0.373..  Exactitud de validación: 0.864\n",
      "Epoch: 13/25..  Número de épocas: 25..  Pérdida de entrenamiento: 0.367..  Pérdida de validación: 0.367..  Exactitud de validación: 0.866\n",
      "Epoch: 14/25..  Número de épocas: 25..  Pérdida de entrenamiento: 0.360..  Pérdida de validación: 0.363..  Exactitud de validación: 0.870\n",
      "Epoch: 15/25..  Número de épocas: 25..  Pérdida de entrenamiento: 0.353..  Pérdida de validación: 0.370..  Exactitud de validación: 0.870\n",
      "Epoch: 16/25..  Número de épocas: 25..  Pérdida de entrenamiento: 0.348..  Pérdida de validación: 0.356..  Exactitud de validación: 0.871\n",
      "Epoch: 17/25..  Número de épocas: 25..  Pérdida de entrenamiento: 0.341..  Pérdida de validación: 0.349..  Exactitud de validación: 0.877\n",
      "Epoch: 18/25..  Número de épocas: 25..  Pérdida de entrenamiento: 0.336..  Pérdida de validación: 0.354..  Exactitud de validación: 0.868\n",
      "Epoch: 19/25..  Número de épocas: 25..  Pérdida de entrenamiento: 0.331..  Pérdida de validación: 0.342..  Exactitud de validación: 0.875\n",
      "Epoch: 20/25..  Número de épocas: 25..  Pérdida de entrenamiento: 0.326..  Pérdida de validación: 0.339..  Exactitud de validación: 0.878\n",
      "Epoch: 21/25..  Número de épocas: 25..  Pérdida de entrenamiento: 0.321..  Pérdida de validación: 0.345..  Exactitud de validación: 0.875\n",
      "Epoch: 22/25..  Número de épocas: 25..  Pérdida de entrenamiento: 0.317..  Pérdida de validación: 0.342..  Exactitud de validación: 0.880\n",
      "Epoch: 23/25..  Número de épocas: 25..  Pérdida de entrenamiento: 0.314..  Pérdida de validación: 0.344..  Exactitud de validación: 0.876\n",
      "Epoch: 24/25..  Número de épocas: 25..  Pérdida de entrenamiento: 0.307..  Pérdida de validación: 0.342..  Exactitud de validación: 0.877\n",
      "Epoch: 25/25..  Número de épocas: 25..  Pérdida de entrenamiento: 0.305..  Pérdida de validación: 0.335..  Exactitud de validación: 0.883\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.0001\n",
    "batch_size = 64\n",
    "\n",
    "results = []\n",
    "\n",
    "for i, epochs in enumerate(n_epocas_values):\n",
    "    print(f\"Experimento {i+1}: Evaluando número de épocas {epochs}\")\n",
    "    model = RedNeuronal(784, 10, [516, 256], drop_p=0.5)\n",
    "    criterion = nn.NLLLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    print_every = 40\n",
    "    \n",
    "    for e in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0\n",
    "        for images, labels in trainloader:\n",
    "            images.resize_(images.size()[0], 784)\n",
    "            optimizer.zero_grad()\n",
    "            output = model.forward(images)\n",
    "            loss = criterion(output, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            test_loss, accuracy = validation(model, validationloader, criterion)\n",
    "        print(f\"Epoch: {e+1}/{epochs}.. \",\n",
    "              f\"Número de épocas: {epochs}.. \",\n",
    "              f\"Pérdida de entrenamiento: {running_loss/len(trainloader):.3f}.. \",\n",
    "              f\"Pérdida de validación: {test_loss/len(validationloader):.3f}.. \",\n",
    "              f\"Exactitud de validación: {accuracy/len(validationloader):.3f}\")\n",
    "        results.append([learning_rate, epochs, running_loss/len(trainloader), test_loss/len(validationloader), accuracy/len(validationloader)])\n",
    "        model.train()\n",
    "\n",
    "\n",
    "df_results = pd.DataFrame(results, columns=['Learning Rate', 'Epochs', 'Training Loss', 'Validation Loss', 'Validation Accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mejor número de épocas:\n",
      "25\n",
      "\n",
      "Resultados asociados al mejor valor:\n",
      "Learning Rate                  0.0001\n",
      "Epochs                             25\n",
      "Training Loss                0.305473\n",
      "Validation Loss              0.334719\n",
      "Validation Accuracy    tensor(0.8825)\n",
      "Name: 74, dtype: object\n"
     ]
    }
   ],
   "source": [
    "df_last_epoch = df_results[df_results.groupby('Epochs')['Epochs'].transform('max') == df_results['Epochs']]\n",
    "best_result = df_last_epoch.loc[df_last_epoch['Validation Accuracy'].idxmax()]\n",
    "\n",
    "print(\"\\nMejor número de épocas:\")\n",
    "print(best_result['Epochs'])\n",
    "print(\"\\nResultados asociados al mejor valor:\")\n",
    "print(best_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusiones\n",
    "\n",
    "¿Cual fue el mejor valor encontrado?\n",
    "¿Cuantas ejecuciones se realizaron?\n",
    "¿Que tiempo tomó realizar todos los experimentos?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mejor valor encontrado:\n",
    "El mejor valor encontrado en los experimentos fue con una tasa de aprendizaje de **0.0001**, un tamaño de lote de **64** y **25** épocas. Este conjunto de hiperparámetros resultó en la mejor exactitud de validación.\n",
    "\n",
    "### Número de ejecuciones realizadas:\n",
    "En total, se realizaron **15** ejecuciones experimentales:\n",
    "- **5** ejecuciones variando la tasa de aprendizaje.\n",
    "- **5** ejecuciones variando el tamaño del lote.\n",
    "- **5** ejecuciones variando el número de épocas.\n",
    "\n",
    "### Tiempo total de los experimentos:\n",
    "El tiempo total para realizar todos los experimentos fue de aproximadamente **35 minutos y 50 segundos**, desglosado de la siguiente manera:\n",
    "- Experimentos variando la tasa de aprendizaje: **11 minutos y 54 segundos**.\n",
    "- Experimentos variando el tamaño del lote: **11 minutos y 13 segundos**.\n",
    "- Experimentos variando el número de épocas: **12 minutos y 43 segundos**.\n",
    "\n",
    "Estos resultados muestran que la red neuronal alcanza su mejor rendimiento con una configuración específica de hiperparámetros y que el tiempo de entrenamiento puede variar dependiendo de los valores de los hiperparámetros utilizados. La configuración óptima hallada puede ser utilizada como referencia para futuros modelos y tareas similares, optimizando así tanto el tiempo de entrenamiento como la precisión del modelo.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El diseño de experimentos es crucial en el entrenamiento de redes neuronales para identificar configuraciones óptimas de hiperparámetros. Utilizar la metodología de variar una variable a la vez (OVAT) ofrece varias ventajas significativas. En primer lugar, proporciona simplicidad y claridad al permitir observar el efecto directo de un solo hiperparámetro en el rendimiento del modelo, facilitando así la comprensión y el ajuste de los parámetros.\n",
    "\n",
    "Además, OVAT reduce la complejidad computacional y es ideal para la optimización iterativa, permitiendo ajustes progresivos basados en los resultados obtenidos. Esto mejora la eficiencia de los programadores en proyectos de Deep Learning, al simplificar el proceso de optimización y permitir una mejor gestión de los recursos computacionales.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_1_12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
